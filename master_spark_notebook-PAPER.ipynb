{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import to_date, col, lit, unix_timestamp\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, PCA\n",
    "from pyspark.sql.functions import to_timestamp, date_format, hour, year, month, dayofmonth\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-12-180.us-west-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._jsc.sc().getExecutorMemoryStatus().size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mongo Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema of the Green Taxi Database - data was cleaned in mongo:\n",
    "\n",
    "```\n",
    "s3cmd get s3://spark-proj/data/yellow_tripdata_2017-05.csv s3://spark-proj/data/yellow_tripdata_2017-04.csv s3://spark-proj/data/yellow_tripdata_2017-03.csv s3://spark-proj/data/yellow_tripdata_2017-02.csv s3://spark-proj/data/yellow_tripdata_2017-01.csv s3://spark-proj/data/yellow_tripdata_2016-12.csv s3://spark-proj/data/yellow_tripdata_2016-11.csv s3://spark-proj/data/yellow_tripdata_2016-10.csv s3://spark-proj/data/yellow_tripdata_2016-09.csv s3://spark-proj/data/yellow_tripdata_2016-08.csv s3://spark-proj/data/green_tripdata_2017-05.csv s3://spark-proj/data/green_tripdata_2017-04.csv s3://spark-proj/data/green_tripdata_2017-03.csv s3://spark-proj/data/green_tripdata_2017-02.csv s3://spark-proj/data/green_tripdata_2017-01.csv s3://spark-proj/data/green_tripdata_2016-12.csv s3://spark-proj/data/green_tripdata_2016-11.csv s3://spark-proj/data/green_tripdata_2016-10.csv s3://spark-proj/data/green_tripdata_2016-09.csv s3://spark-proj/data/green_tripdata_2016-08.csv\n",
    "\n",
    "mongoimport -d taxidb -c taxidata --type csv --headerline --file yellow_tripdata_2017-05.csv\n",
    "mongoimport -d taxidb -c taxidata --type csv --headerline --file yellow_tripdata_2017-04.csv\n",
    "mongoimport -d taxidb -c taxidata --type csv --headerline --file yellow_tripdata_2017-03.csv\n",
    "mongoimport -d taxidb -c taxidata --type csv --headerline --file yellow_tripdata_2017-02.csv\n",
    "mongoimport -d taxidb -c taxidata --type csv --headerline --file yellow_tripdata_2017-01.csv\n",
    "\n",
    "mongoimport -d taxidb -c taxidata --type csv --headerline --file green_tripdata_2017-05.csv\n",
    "mongoimport -d taxidb -c taxidata --type csv --headerline --file green_tripdata_2017-04.csv\n",
    "mongoimport -d taxidb -c taxidata --type csv --headerline --file green_tripdata_2017-03.csv\n",
    "mongoimport -d taxidb -c taxidata --type csv --headerline --file green_tripdata_2017-02.csv\n",
    "mongoimport -d taxidb -c taxidata --type csv --headerline --file green_tripdata_2017-01.csv\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Green Taxi Data from Mongo Database ( 1M total)\n",
    "\n",
    "Our data is currently stored in another EC2 database:\n",
    "\n",
    "`mongodb://ec2-54-190-6-201.us-west-2.compute.amazonaws.com/taxidb_sm.taxidata_g`\n",
    "\n",
    "- 10M : mongodb://ec2-35-162-204-59.us-west-2.compute.amazonaws.com/taxidb.taxidata10M_y\n",
    "- 30M : mongodb://ec2-35-162-204-59.us-west-2.compute.amazonaws.com/taxidb.taxidata30M_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.41381812096\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "green_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"uri\",\"mongodb://ec2-35-162-204-59.us-west-2.compute.amazonaws.com/taxidb.taxidata30M_g\")\\\n",
    "    .load()\n",
    "green_df.repartition(24)\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- color: double (nullable = true)\n",
      " |-- ehail_fee: string (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "green_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data ( per day per hour) - weather underground.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# weather_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://ec2-54-190-6-201.us-west-2.compute.amazonaws.com/taxidb.weather_data\").load()\n",
    "# print time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexStringColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# w_df = indexStringColumns(weather_df, ['condition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# w_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yellow Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.37875199318\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "yellow_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"uri\",\"mongodb://ec2-35-162-204-59.us-west-2.compute.amazonaws.com/taxidb.taxidata30M_y\")\\\n",
    "    .load()\n",
    "yellow_df.repartition(24)\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- color: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellow_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE SAMPLE SIZES\n",
    "\n",
    "- 0.3 = 3M\n",
    "- 0.1 = 1M\n",
    "- 1 = 10M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue Yellow and Green Taxi data together for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------+--------+-----+-----+-----------+---------------------+-------+---------------+------------+------------------+----------+------------+------------+---------------------+--------------------+-------------+\n",
      "|DOLocationID|PULocationID|RatecodeID|VendorID|color|extra|fare_amount|improvement_surcharge|mta_tax|passenger_count|payment_type|store_and_fwd_flag|tip_amount|tolls_amount|total_amount|tpep_dropoff_datetime|tpep_pickup_datetime|trip_distance|\n",
      "+------------+------------+----------+--------+-----+-----+-----------+---------------------+-------+---------------+------------+------------------+----------+------------+------------+---------------------+--------------------+-------------+\n",
      "|         144|         142|         1|       1|  1.0|  0.5|       16.0|                  0.3|    0.5|              1|           2|                 N|       0.0|         0.0|        17.3|  2017-05-01 00:18:49| 2017-05-01 00:00:02|          4.1|\n",
      "|         233|         138|         1|       2|  1.0|  0.5|       24.5|                  0.3|    0.5|              5|           1|                 N|       4.0|        5.76|       35.56|  2017-05-01 00:17:01| 2017-05-01 00:00:03|         8.41|\n",
      "+------------+------------+----------+--------+-----+-----+-----------+---------------------+-------+---------------+------------+------------------+----------+------------+------------+---------------------+--------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_cols = [x for x in yellow_df.columns if x!='_id' ]\n",
    "\n",
    "y_df = yellow_df.select(select_cols)\n",
    "#y_df = y_df.sample(False,0.05, seed=1)\n",
    "y_df.cache()\n",
    "\n",
    "g_df = green_df.select(select_cols)\n",
    "#g_df = g_df.sample(False,0.05, seed=1)\n",
    "g_df.cache()\n",
    "\n",
    "total_df = y_df.unionAll(g_df)\n",
    "total_df.repartition(24)\n",
    "total_df.cache()\n",
    "total_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - extracting data from dates\n",
    "\n",
    "- Day of Week\n",
    "- Day of Month\n",
    "- Month\n",
    "- Round to hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dates(df, colName, newCol):\n",
    "    return df.withColumn(newCol,to_timestamp(colName, 'yyyy-MM-dd HH:mm:ss')).drop(colName)\n",
    "\n",
    "def get_dateinfo(df, colName):\n",
    "    return df.withColumn('dow', date_format(colName,'u').cast(IntegerType()))\\\n",
    "    .withColumn('hour', hour(colName))\\\n",
    "    .withColumn('day', dayofmonth(colName))\\\n",
    "    .withColumn('month', month(colName))\\\n",
    "    .withColumn('year', year(colName))\n",
    "    \n",
    "def indexStringColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the total dataset\n",
    "- 100k : 73.8127279282\n",
    "- 500k : 75.45023489\n",
    "- 1M : 54.37\n",
    "- 3M : 54.6 Secs\n",
    "- 10M : 59.06 secs\n",
    "- 30M : 166.949352026\n",
    "- 70M :\n",
    "- 100M :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148.511734962\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "timeDiff = (unix_timestamp('tpep_dropoff_datetime') - unix_timestamp('tpep_pickup_datetime'))\n",
    "df_dates = total_df.withColumn(\"Duration\", timeDiff)\n",
    "df_dates = df_dates.filter(\"Duration < 7200 and Duration > 60\" )\n",
    "df_dates = make_dates(total_df, \"tpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "df_dates = make_dates(df_dates, \"tpep_pickup_datetime\", \"pickup_datetime\")\n",
    "new_df = indexStringColumns(df_dates, [\"store_and_fwd_flag\"])\n",
    "new_df = get_dateinfo(df_dates, 'dropoff_datetime')\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join in the weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# joined_df = new_df.join(w_df,[new_df.day==w_df.day, new_df.month==w_df.month, new_df.hour==w_df.hour], 'left_outer')\n",
    "# joined_df = joined_df.select(new_df.DOLocationID, new_df.PULocationID, new_df.RatecodeID, new_df.VendorID, new_df.color, new_df.extra, new_df.fare_amount, new_df.improvement_surcharge, new_df.mta_tax, new_df.passenger_count, new_df.payment_type, new_df.store_and_fwd_flag, new_df.tip_amount, new_df.tolls_amount, new_df.total_amount, new_df.trip_distance, new_df.dropoff_datetime, new_df.pickup_datetime, new_df.dow, new_df.hour, new_df.day, new_df.month, new_df.year, w_df.temp, w_df.condition)\n",
    "joined_df = new_df\n",
    "joined_df = joined_df.drop(\"dropoff_datetime\").drop(\"pickup_datetime\").drop(\"store_and_fwd_flag\").drop(\"ehail_fee\").drop(\"trip_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encode any of the categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- color: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- dow: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- DOLocationID: vector (nullable = true)\n",
      " |-- PULocationID: vector (nullable = true)\n",
      " |-- RatecodeID: vector (nullable = true)\n",
      " |-- VendorID: vector (nullable = true)\n",
      " |-- dow: vector (nullable = true)\n",
      " |-- day: vector (nullable = true)\n",
      " |-- month: vector (nullable = true)\n",
      "\n",
      "28.7997457981\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "\n",
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        #For each given colum, create OneHotEncoder. \n",
    "        #dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf\n",
    "\n",
    "onehot_cols = [ x for x in joined_df.columns if 'ID' in x]\n",
    "onehot_cols = onehot_cols + ['dow','day','month'] # took out weather + condition\n",
    "\n",
    "joined_df.printSchema()\n",
    "dfhot = oneHotEncodeColumns(joined_df,onehot_cols)\n",
    "df_for_model = dfhot.withColumnRenamed(\"color\",\"label\")\n",
    "df_for_model.printSchema()\n",
    "\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Vectors for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "va = VectorAssembler(outputCol=\"features\", inputCols=df_for_model.drop(\"label\").columns) #except the last col.\n",
    "taxi_points = va.transform(df_for_model).select(\"features\", \"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_data = taxi_points.randomSplit([0.8, 0.2])\n",
    "training = split_data[0].cache()\n",
    "test = split_data[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARKING BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 100k : 36.5315189362 | 0.91292 | 0.909347\n",
    "- 500k : 51.1424059868 | 0.909106 | 0.907606\n",
    "- 1M : 42.5615952015 | 0.909293 | 0.908242\n",
    "- 3M : 76.5072159767 | 0.907866 | 0.907026\n",
    "- 10M : 247.885142088 | 0.910769 | 0.910489\n",
    "- 30M : 815.310534 | 0.90917 | 0.909082\n",
    "- 70M :\n",
    "- 100M :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rf = RandomForestClassifier(maxDepth=15)\n",
    "rfmodel = rf.fit(training)\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rfpredicts_train = rfmodel.transform(training)\n",
    "rfpredicts_test = rfmodel.transform(test)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "train_accuracy = evaluator.evaluate(rfpredicts_train)\n",
    "test_accuracy = evaluator.evaluate(rfpredicts_test)\n",
    "\n",
    "print(\"train / test Accuracy = %g | %g\" % (train_accuracy, test_accuracy))\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import operator\n",
    "\n",
    "# columns = df_for_model.drop(\"label\").columns\n",
    "# importances = {}\n",
    "# for score,col in zip(rfmodel.featureImportances, columns):\n",
    "#     importances[col] =  score\n",
    "# sorted_importances = sorted(importances.items(), key=operator.itemgetter(1), reverse = True)\n",
    "# sorted_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "- 100K : 14.2321848869 | 0.963656 | 0.960007\n",
    "- 500K : 16.9678959846 | 0.96247 | 0.962208\n",
    "- 1M : 12.0236101151 | 0.962448 | 0.961711\n",
    "- 3M : 19.3087320328 | 0.962437 | 0.962179\n",
    "- 10M : 37.7062170506 | 0.962361 | 0.962184\n",
    "- 30M : 357.43987608| 0.961334 | 0.961369\n",
    "- 70M :\n",
    "- 100M :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.9678959846\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "logreg = LogisticRegression()\n",
    "log_model = logreg.fit(training)\n",
    "print time.time() - start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train / test Accuracy = 0.96247 | 0.962208\n",
      "1.42913389206\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "logpredicts_test = log_model.transform(test)\n",
    "logpredicts_train = log_model.transform(training)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "train_accuracy = evaluator.evaluate(logpredicts_train)\n",
    "test_accuracy = evaluator.evaluate(logpredicts_test)\n",
    "\n",
    "print(\"train / test Accuracy = %g | %g\" % (train_accuracy, test_accuracy))\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Kmeans\n",
    "- 100K: 7.25322890282\n",
    "- 500K: 13.6290647984\n",
    "- 1M : 11.3732612133\n",
    "- 3M : 29.7429788113\n",
    "- 10M : 84.9775218964\n",
    "- 30M : 194.982362986\n",
    "- 70M :\n",
    "- 100M :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|prediction|   count|\n",
      "+----------+--------+\n",
      "|         1|       1|\n",
      "|         6| 1129493|\n",
      "|         3|       1|\n",
      "|         5|       1|\n",
      "|         9| 9809590|\n",
      "|         4| 2123725|\n",
      "|         8| 6654671|\n",
      "|         7|      10|\n",
      "|         2|       2|\n",
      "|         0|14025318|\n",
      "+----------+--------+\n",
      "\n",
      "None\n",
      "194.982362986\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "kmean_df = df_for_model.withColumn(\"label\", lit(0))\n",
    "\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=kmean_df.drop(\"label\").columns) #except the last col.\n",
    "taxi_points_mean = va.transform(kmean_df).select(\"features\", \"label\")\n",
    "\n",
    "kmeans = KMeans().setK(10).setFeaturesCol(\"features\").setPredictionCol(\"prediction\")\n",
    "model = kmeans.fit(taxi_points_mean).transform(taxi_points_mean)\n",
    "\n",
    "print model.groupBy(\"prediction\").count().show()\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF Regression\n",
    "- 100K : 42.839509964 | 0.226576 | 0.214108\n",
    "- 500K : 99.1277749538 | 0.226455 | 0.220842\n",
    "- 1M : 106.496699095 | 0.227120 | 0.222687\n",
    "- 3M : 240.4 | 0.225 | 0.223 \n",
    "- 10M : 628.436889887 | 0.226972 | 0.225364\n",
    "- 30M :\n",
    "- 70M :\n",
    "- 100M :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# then do all that stuff\n",
    "df_travel = df_for_model.withColumnRenamed(\"Duration\", \"label\")\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=df_travel.drop(\"label\").columns) #except the last col.\n",
    "taxi_points_travel = va.transform(df_travel).select(\"features\", \"label\")\n",
    "\n",
    "split_data = taxi_points_travel.randomSplit([0.8, 0.2])\n",
    "training = split_data[0].cache()\n",
    "test = split_data[1].cache()\n",
    "\n",
    "rf_r = RandomForestRegressor(maxDepth=15)\n",
    "rfmodel = rf_r.fit(training)\n",
    "\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfreg_predicts_test = rfmodel.transform(test)\n",
    "rfreg_predicts_training = rfmodel.transform(training)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "test_rmse = evaluator.evaluate(rfreg_predicts_test)\n",
    "train_rmse = evaluator.evaluate(rfreg_predicts_training)\n",
    "print(\"train / test RMSE = %f | %f\" % (test_rmse, train_rmse))\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA \n",
    "- 100k: 3.53512597084\n",
    "- 500k: 10.2684950829\n",
    "- 1M : 7.92126703262\n",
    "- 3M : 21.8753027916\n",
    "- 10M : 65.5080680847\n",
    "- 30M :\n",
    "- 70M :\n",
    "- 100M :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148.88379097\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(taxi_points)\n",
    "result = model.transform(taxi_points).select(\"pcaFeatures\")\n",
    "\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient Regression\n",
    "\n",
    "- 100k:  218.333948135 | 0.190186 | 0.125398\n",
    "- 500k: \n",
    "- 1M : \n",
    "- 3M : \n",
    "- 10M :\n",
    "- 30M :\n",
    "- 70M :\n",
    "- 100M :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218.333948135\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# then do all that stuff\n",
    "df_travel = df_for_model.withColumnRenamed(\"Duration\", \"label\")\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=df_travel.drop(\"label\").columns) #except the last col.\n",
    "taxi_points_travel = va.transform(df_travel).select(\"features\", \"label\")\n",
    "\n",
    "split_data = taxi_points_travel.randomSplit([0.8, 0.2])\n",
    "training = split_data[0].cache()\n",
    "test = split_data[1].cache()\n",
    "\n",
    "gb_r = GBTRegressor(maxDepth=15)\n",
    "gbmodel = gb_r.fit(training)\n",
    "\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train / test RMSE = 0.190186 | 0.125398\n",
      "220.115098\n"
     ]
    }
   ],
   "source": [
    "gbreg_predicts_test = gbmodel.transform(test)\n",
    "gbreg_predicts_training = gbmodel.transform(training)\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "test_rmse = evaluator.evaluate(gbreg_predicts_test)\n",
    "train_rmse = evaluator.evaluate(gbreg_predicts_training)\n",
    "print(\"train / test RMSE = %f | %f\" % (test_rmse, train_rmse))\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient Classification\n",
    "\n",
    "- 100k: 319.882117987 | 0.979915 | 0.951964\n",
    "- 500k: \n",
    "- 1M : \n",
    "- 3M : \n",
    "- 10M :\n",
    "- 30M :\n",
    "- 70M :\n",
    "- 100M :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319.882117987\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "gbc = GBTClassifier(maxDepth=15)\n",
    "gbcmodel = gbc.fit(training)\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train / test Accuracy = 0.979915 | 0.951964\n",
      "4.33535385132\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "gbcpredicts_train = gbcmodel.transform(training)\n",
    "gbcpredicts_test = gbcmodel.transform(test)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "train_accuracy = evaluator.evaluate(gbcpredicts_train)\n",
    "test_accuracy = evaluator.evaluate(gbcpredicts_test)\n",
    "\n",
    "print(\"train / test Accuracy = %g | %g\" % (train_accuracy, test_accuracy))\n",
    "print time.time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEEP A STANDARD DATASIZE FOR THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Benchmarks (by tree)\n",
    "\n",
    "- 1M :\n",
    "- 3M :\n",
    "- 10M :\n",
    "- 30M :\n",
    "- 70M :\n",
    "- 100M :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# then do all that stuff\n",
    "df_travel = df_for_model.withColumnRenamed(\"Duration\", \"label\")\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=df_travel.drop(\"label\").columns) #except the last col.\n",
    "taxi_points_travel = va.transform(df_travel).select(\"features\", \"label\")\n",
    "\n",
    "split_data = taxi_points_travel.randomSplit([0.8, 0.2])\n",
    "training = split_data[0].cache()\n",
    "test = split_data[1].cache()\n",
    "\n",
    "for md in [3,6,9,12,15,18]\n",
    "    start = time.time()\n",
    "\n",
    "    rf_r = RandomForestRegressor(maxDepth=3)\n",
    "    rfmodel = rf_r.fit(training)\n",
    "\n",
    "\n",
    "    print time.time() - start"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
