{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import to_date, col, lit, unix_timestamp\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, PCA\n",
    "from pyspark.sql.functions import to_timestamp, date_format, hour, year, month, dayofmonth\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor, LinearRegression\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexStringColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf\n",
    "\n",
    "def make_dates(df, colName, newCol):\n",
    "    return df.withColumn(newCol,to_timestamp(colName, 'yyyy-MM-dd HH:mm:ss')).drop(colName)\n",
    "\n",
    "def get_dateinfo(df, colName):\n",
    "    return df.withColumn('dow', date_format(colName,'u').cast(IntegerType()))\\\n",
    "    .withColumn('hour', hour(colName))\\\n",
    "    .withColumn('day', dayofmonth(colName))\\\n",
    "    .withColumn('month', month(colName))\\\n",
    "    .withColumn('year', year(colName))\n",
    "    \n",
    "\n",
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        #For each given colum, create OneHotEncoder. \n",
    "        #dropLast : Whether to drop the last category in the encoded vector (default: true)\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        #Creates a DataFame by putting the transformed values in the new colum with suffix \"-onehot\" \n",
    "        #and then drops the original columns.\n",
    "        #and drop the \"-onehot\" suffix. \n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Study Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_green_data(sz='30M'):\n",
    "    start = time.time()\n",
    "    if sz == '30M':\n",
    "        uri = \"mongodb://ec2-35-162-204-59.us-west-2.compute.amazonaws.com/taxidb.taxidata30M_g\"\n",
    "    else:\n",
    "        uri = \"mongodb://ec2-35-162-204-59.us-west-2.compute.amazonaws.com/taxidb.taxidata10M_y\"\n",
    "    \n",
    "    green_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"uri\",uri)\\\n",
    "        .load()\n",
    "    green_df.repartition(24)\n",
    "    \n",
    "    select_cols = [x for x in green_df.columns if x not in ['_id','trip_type', 'ehail_fee'] ]\n",
    "    g_df = green_df.select(select_cols)\n",
    "    \n",
    "    if sz == '3M':\n",
    "        g_df = g_df.sample(False,0.3, seed=1)\n",
    "    elif sz == '1M':\n",
    "        g_df = g_df.sample(False,0.1, seed=1)\n",
    "    elif sz == '500K':\n",
    "        g_df = g_df.sample(False,0.05, seed=1)        \n",
    "    elif sz == '100K':\n",
    "        g_df = g_df.sample(False,0.01, seed=1)                \n",
    "    elif sz == '10K':\n",
    "        g_df = g_df.sample(False,0.001, seed=1)      \n",
    "\n",
    "    g_df.cache()\n",
    "    print '%s green data loaded...' %sz, ( time.time() - start)\n",
    "\n",
    "    return g_df\n",
    "\n",
    "def get_yellow_data(sz='30M'):\n",
    "    start = time.time()\n",
    "    if sz == '30M':\n",
    "        uri = \"mongodb://ec2-35-162-204-59.us-west-2.compute.amazonaws.com/taxidb.taxidata30M_y\"\n",
    "    else:\n",
    "        uri = \"mongodb://ec2-35-162-204-59.us-west-2.compute.amazonaws.com/taxidb.taxidata10M_g\"\n",
    "        \n",
    "    yellow_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"uri\",uri)\\\n",
    "        .load()\n",
    "    yellow_df.repartition(24)\n",
    "    \n",
    "    select_cols = [x for x in yellow_df.columns if x not in ['_id','trip_type', 'ehail_fee'] ]\n",
    "    y_df = yellow_df.select(select_cols)\n",
    "    \n",
    "    if sz == '3M':\n",
    "        y_df = y_df.sample(False,0.3, seed=1)\n",
    "    elif sz == '1M':\n",
    "        y_df = y_df.sample(False,0.1, seed=1)\n",
    "    elif sz == '500K':\n",
    "        y_df = y_df.sample(False,0.05, seed=1)        \n",
    "    elif sz == '100K':\n",
    "        y_df = y_df.sample(False,0.01, seed=1)                \n",
    "    elif sz == '10K':\n",
    "        y_df = y_df.sample(False,0.001, seed=1)                        \n",
    "        \n",
    "        \n",
    "    y_df.cache()\n",
    "    print '%s yellow data loaded...' %sz, ( time.time() - start)\n",
    "    return y_df\n",
    "    \n",
    "    \n",
    "def concat_data_set(sz='30M'):\n",
    "    print 'combining %s datasets ...' %sz\n",
    "    g_df = get_green_data(sz)\n",
    "    y_df = get_yellow_data(sz)\n",
    "    \n",
    "    total_df = y_df.unionAll(g_df)\n",
    "    print 'repartitioning to 24 partitions ...'\n",
    "    total_df.repartition(24)\n",
    "    total_df.cache()\n",
    "    return total_df\n",
    "\n",
    "def feature_eng(total_df):\n",
    "    start = time.time()\n",
    "    print 'starting feature engineering...'\n",
    "    timeDiff = (unix_timestamp('tpep_dropoff_datetime') - unix_timestamp('tpep_pickup_datetime'))\n",
    "    df_dates = total_df.withColumn(\"Duration\", timeDiff)\n",
    "    df_dates = df_dates.filter(\"Duration < 7200 and Duration > 60\" )\n",
    "    df_dates = make_dates(df_dates, \"tpep_dropoff_datetime\", \"dropoff_datetime\")\n",
    "    df_dates = make_dates(df_dates, \"tpep_pickup_datetime\", \"pickup_datetime\")\n",
    "    new_df = indexStringColumns(df_dates, [\"store_and_fwd_flag\"])\n",
    "    new_df = get_dateinfo(new_df, 'dropoff_datetime')\n",
    "    new_df = new_df.drop(\"dropoff_datetime\").drop(\"pickup_datetime\").drop(\"store_and_fwd_flag\").drop(\"ehail_fee\").drop(\"trip_type\")\n",
    "    \n",
    "    \n",
    "    onehot_cols = [ x for x in new_df.columns if 'ID' in x]\n",
    "    onehot_cols = onehot_cols + ['dow','day','month'] # took out weather + condition\n",
    "\n",
    "    dfhot = oneHotEncodeColumns(new_df,onehot_cols)\n",
    "    \n",
    "    print 'feature eng complete ... ', time.time() - start\n",
    "    return dfhot\n",
    "\n",
    "def df2vec(df_in, study_type = 'classification'):\n",
    "    if study_type == 'classification':\n",
    "        df_for_model = df_in.withColumnRenamed(\"color\",\"label\")\n",
    "        va = VectorAssembler(outputCol=\"features\", inputCols=df_for_model.drop(\"label\").columns) #except the last col.\n",
    "        taxi_points = va.transform(df_for_model).select(\"features\", \"label\")  \n",
    "    else:\n",
    "        df_travel = df_in.withColumnRenamed(\"Duration\", \"label\")\n",
    "        va = VectorAssembler(outputCol=\"features\", inputCols=df_travel.drop(\"label\").columns) #except the last col.\n",
    "        taxi_points = va.transform(df_travel).select(\"features\", \"label\")\n",
    "\n",
    "    split_data = taxi_points.randomSplit([0.8, 0.2])\n",
    "    training = split_data[0].cache()\n",
    "    test = split_data[1].cache()\n",
    "    \n",
    "    print 'data converted into train and test vecs...'\n",
    "    return training, test\n",
    "\n",
    "\n",
    "def run_model(df_for_model, sz='30M',model='rfc'):\n",
    "    if model in ('rfc', 'logreg'):\n",
    "        training, test = df2vec(df_for_model, study_type = 'classification')\n",
    "    else:\n",
    "        training, test = df2vec(df_for_model, study_type = 'regression')\n",
    "    \n",
    "    print 'creating model and fitting ...'\n",
    "    start = time.time()\n",
    "    if model == 'rfc':\n",
    "        m = RandomForestClassifier(maxDepth=15, maxMemoryInMB=2048)\n",
    "    elif model == 'linreg':\n",
    "        m = LinearRegression(maxIter=20, regParam=0.3, elasticNetParam=0.8)\n",
    "    elif model == 'rfr':\n",
    "        m = RandomForestRegressor(maxDepth=15, maxMemoryInMB=2048)\n",
    "    elif model == 'logreg':\n",
    "        m = LogisticRegression()\n",
    "    elif model == 'PCA':\n",
    "        m = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\") \n",
    "    elif model == 'Kmeans':\n",
    "        m = KMeans().setK(10).setFeaturesCol(\"features\").setPredictionCol(\"prediction\")\n",
    " \n",
    "    m_fit = m.fit(training)\n",
    "    print '='*20 + '%s  MODEL RESULTS' % model + '='*20\n",
    "    print '%s for %s fitting time: %f' % (sz, model, time.time() - start)\n",
    "    start = time.time()\n",
    "    \n",
    "    if model in ('rfc','rfr', 'logreg','linreg'):\n",
    "        predicts_train = m_fit.transform(training)\n",
    "        predicts_test = m_fit.transform(test)\n",
    "        print '%s for %s prediction time: %f' % (sz, model, time.time() - start)\n",
    "        \n",
    "    if model in ('rfc', 'logreg'):\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        train_accuracy = evaluator.evaluate(predicts_train)\n",
    "        test_accuracy = evaluator.evaluate(predicts_test)\n",
    "        print \"train acc %g\" % train_accuracy\n",
    "        print \"train acc %g\" % test_accuracy\n",
    "        \n",
    "    elif model in ('rfr', 'linreg'):\n",
    "        evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        test_rmse = evaluator.evaluate(predicts_test)\n",
    "        train_rmse = evaluator.evaluate(predicts_train)\n",
    "        print \"train RMSE = %f \" % test_rmse\n",
    "        print \"train RMSE = %f \" % train_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = '10K'\n",
    "total_df = concat_data_set(sz)\n",
    "df_for_model = feature_eng(total_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lin Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_model(df_for_model, sz=sz, model='linreg')\n",
    "run_model(df_for_model, sz=sz, model='linreg')\n",
    "run_model(df_for_model, sz=sz, model='linreg')\n",
    "run_model(df_for_model, sz=sz, model='linreg')\n",
    "run_model(df_for_model, sz=sz, model='linreg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(df_for_model, sz=sz, model='PCA')\n",
    "run_model(df_for_model, sz=sz, model='PCA')\n",
    "run_model(df_for_model, sz=sz, model='PCA')\n",
    "run_model(df_for_model, sz=sz, model='PCA')\n",
    "run_model(df_for_model, sz=sz, model='PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================Kmeans  MODEL RESULTS====================\n",
      "10M for Kmeans fitting time: 49.369925\n",
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================Kmeans  MODEL RESULTS====================\n",
      "10M for Kmeans fitting time: 49.032614\n",
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================Kmeans  MODEL RESULTS====================\n",
      "10M for Kmeans fitting time: 48.937607\n",
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================Kmeans  MODEL RESULTS====================\n",
      "10M for Kmeans fitting time: 49.419997\n",
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================Kmeans  MODEL RESULTS====================\n",
      "10M for Kmeans fitting time: 49.064854\n"
     ]
    }
   ],
   "source": [
    "run_model(df_for_model, sz=sz, model='Kmeans')\n",
    "run_model(df_for_model, sz=sz, model='Kmeans')\n",
    "run_model(df_for_model, sz=sz, model='Kmeans')\n",
    "run_model(df_for_model, sz=sz, model='Kmeans')\n",
    "run_model(df_for_model, sz=sz, model='Kmeans')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================logreg  MODEL RESULTS====================\n",
      "10M for logreg fitting time: 77.556721\n",
      "10M for logreg prediction time: 0.064288\n",
      "train acc 0.963059\n",
      "train acc 0.963012\n",
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================logreg  MODEL RESULTS====================\n",
      "10M for logreg fitting time: 77.552119\n",
      "10M for logreg prediction time: 0.067104\n",
      "train acc 0.963038\n",
      "train acc 0.962879\n",
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================logreg  MODEL RESULTS====================\n",
      "10M for logreg fitting time: 76.361705\n",
      "10M for logreg prediction time: 0.126839\n",
      "train acc 0.96304\n",
      "train acc 0.96286\n",
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================logreg  MODEL RESULTS====================\n",
      "10M for logreg fitting time: 77.914439\n",
      "10M for logreg prediction time: 0.062515\n",
      "train acc 0.963004\n",
      "train acc 0.962959\n",
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================logreg  MODEL RESULTS====================\n",
      "10M for logreg fitting time: 76.468697\n",
      "10M for logreg prediction time: 0.107815\n",
      "train acc 0.962963\n",
      "train acc 0.963007\n"
     ]
    }
   ],
   "source": [
    "run_model(df_for_model, sz=sz, model='logreg')\n",
    "run_model(df_for_model, sz=sz, model='logreg')\n",
    "run_model(df_for_model, sz=sz, model='logreg')\n",
    "run_model(df_for_model, sz=sz, model='logreg')\n",
    "run_model(df_for_model, sz=sz, model='logreg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================rfc  MODEL RESULTS====================\n",
      "10M for rfc fitting time: 175.518291\n",
      "10M for rfc prediction time: 0.112485\n",
      "train acc 0.912708\n",
      "train acc 0.912535\n",
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================rfc  MODEL RESULTS====================\n",
      "10M for rfc fitting time: 183.454627\n",
      "10M for rfc prediction time: 0.121293\n",
      "train acc 0.908774\n",
      "train acc 0.908787\n",
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================rfc  MODEL RESULTS====================\n",
      "10M for rfc fitting time: 180.264990\n",
      "10M for rfc prediction time: 0.115046\n",
      "train acc 0.910676\n",
      "train acc 0.910664\n",
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================rfc  MODEL RESULTS====================\n",
      "10M for rfc fitting time: 181.279588\n",
      "10M for rfc prediction time: 0.110656\n",
      "train acc 0.911879\n",
      "train acc 0.9116\n",
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dff558a9afef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_for_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rfc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_for_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rfc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_for_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rfc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-126955157c23>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(df_for_model, sz, model)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetFeaturesCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetPredictionCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mm_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'%s  MODEL RESULTS'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'%s for %s fitting time: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hadoop/anaconda2/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_model(df_for_model, sz=sz, model='rfc')\n",
    "run_model(df_for_model, sz=sz, model='rfc')\n",
    "run_model(df_for_model, sz=sz, model='rfc')\n",
    "run_model(df_for_model, sz=sz, model='rfc')\n",
    "run_model(df_for_model, sz=sz, model='rfc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n",
      "====================rfr  MODEL RESULTS====================\n",
      "1M for rfr fitting time: 225.974148\n",
      "1M for rfr prediction time: 5.695689\n",
      "train RMSE = 168.202070 \n",
      "train RMSE = 154.970227 \n",
      "data converted into train and test vecs...\n",
      "creating model and fitting ...\n"
     ]
    }
   ],
   "source": [
    "run_model(df_for_model, sz=sz, model='rfr')\n",
    "run_model(df_for_model, sz=sz, model='rfr')\n",
    "run_model(df_for_model, sz=sz, model='rfr')\n",
    "run_model(df_for_model, sz=sz, model='rfr')\n",
    "run_model(df_for_model, sz=sz, model='rfr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
